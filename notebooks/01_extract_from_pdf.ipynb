{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract strands and sub-strands with page numbers in smaller segments\n",
    "def extract_strands_sub_strands_with_page_segmented(pdf_path, start_page, end_page):\n",
    "    extracted_data = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number in range(start_page, min(end_page, len(pdf.pages))):\n",
    "            try:\n",
    "                page = pdf.pages[page_number]\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if \"Strand\" in table[0] and \"Sub Strand\" in table[0]:\n",
    "                        for row in table[1:]:\n",
    "                            if row[0] and row[1]:\n",
    "                                extracted_data.append({\n",
    "                                    \"page_number\": page_number + 1,\n",
    "                                    \"strand\": row[0],\n",
    "                                    \"sub_strand\": row[1]\n",
    "                                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_number + 1}: {e}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Example usage: Process the PDF in segments\n",
    "pdf_path = '../data/raw/GRADE 6 CURRICULUM DESIGNS- MATHEMATICS.pdf'\n",
    "# pdf_path = '../data/raw/GRADE 6 CURRICULUM DESIGNS- HOME SCIENCE.pdf'\n",
    "total_pages = 100  # Assuming the total number of pages in the PDF\n",
    "segment_size = 10  # Number of pages to process at a time\n",
    "\n",
    "all_strands_sub_strands = []\n",
    "for start_page in range(0, total_pages, segment_size):\n",
    "    extracted_data = extract_strands_sub_strands_with_page_segmented(pdf_path, start_page, start_page + segment_size)\n",
    "    all_strands_sub_strands.extend(extracted_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_strands_sub_strands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Page extraction with rubric page [NOT WORKING]\n",
    "def extract_strands_sub_strands_with_page_segmented(pdf_path, start_page, end_page):\n",
    "    extracted_data = []\n",
    "    rubric_headers = [\"Indicators\", \"Exceeds Expectations\", \"Meets Expectations\", \"Approaches Expectations\", \"Below Expectations\"]\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number in range(start_page, min(end_page, len(pdf.pages))):\n",
    "            try:\n",
    "                page = pdf.pages[page_number]\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    # Check if the table is for Strand and Sub Strand\n",
    "                    if \"Strand\" in ' '.join(table[0]) and \"Sub Strand\" in ' '.join(table[0]):\n",
    "                        for row in table[1:]:\n",
    "                            if row[0] and row[1]:\n",
    "                                strand_data = {\n",
    "                                    \"page_number\": page_number + 1,\n",
    "                                    \"strand\": row[0],\n",
    "                                    \"sub_strand\": row[1],\n",
    "                                    \"rubric_start_page\": None  # Default value\n",
    "                                }\n",
    "                                extracted_data.append(strand_data)\n",
    "\n",
    "                    # Check each cell in the first row for rubric headers\n",
    "                    for cell in table[0]:\n",
    "                        if any(header in cell for header in rubric_headers):\n",
    "                            ## print (cell)\n",
    "                            print(cell)\n",
    "                            if extracted_data:\n",
    "                                extracted_data[-1][\"rubric_start_page\"] = page_number + 1\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_number + 1}: {e}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Example usage: Process the PDF in segments\n",
    "pdf_path = '../data/raw/GRADE 6 CURRICULUM DESIGNS- MATHEMATICS.pdf'\n",
    "total_pages = 100  # Assuming the total number of pages in the PDF\n",
    "segment_size = 10  # Number of pages to process at a time\n",
    "\n",
    "all_strands_sub_strands = []\n",
    "for start_page in range(0, total_pages, segment_size):\n",
    "    extracted_data = extract_strands_sub_strands_with_page_segmented(pdf_path, start_page, start_page + segment_size)\n",
    "    all_strands_sub_strands.extend(extracted_data)\n",
    "\n",
    "# Displaying a portion of the extracted data for review\n",
    "print(all_strands_sub_strands)  # Displaying the first 5 entries for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "rubric_headers = [\"Indicators\", \"Exceeds Expectations\", \"Meets Expectations\", \"Approaches Expectations\", \"Below Expectations\"]\n",
    "\n",
    "def find_rubric_start_pages(pdf_path, all_strands_sub_strands, rubric_headers):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for strand_info in all_strands_sub_strands:\n",
    "            start_page = strand_info.get(\"page_number\", 0)  # Default to 0 if not found\n",
    "            found_rubric = False\n",
    "\n",
    "            for page_number in range(start_page, len(pdf.pages)):\n",
    "                page = pdf.pages[page_number]\n",
    "                tables = page.extract_tables()\n",
    "\n",
    "                for table in tables:\n",
    "                    first_row_text = ' '.join(cell for cell in table[0] if cell)\n",
    "                    if any(header in first_row_text for header in rubric_headers):\n",
    "                        strand_info[\"rubric_start_page\"] = page_number + 1  # Page numbers are 1-indexed\n",
    "                        found_rubric = True\n",
    "                        break  # Found the rubric, no need to check further\n",
    "\n",
    "                if found_rubric:\n",
    "                    break  # Move to the next strand_info\n",
    "\n",
    "            if not found_rubric:\n",
    "                strand_info[\"rubric_start_page\"] = None  # Set to None if no rubric page is found\n",
    "\n",
    "    return all_strands_sub_strands\n",
    "\n",
    "\n",
    "\n",
    "# Run the function with the provided data\n",
    "updated_strands_sub_strands = find_rubric_start_pages(pdf_path, all_strands_sub_strands, rubric_headers)\n",
    "\n",
    "# Displaying a portion of the updated data\n",
    "print(updated_strands_sub_strands)  # Displaying the first 5 entries for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORKING\n",
    "complete_data = None\n",
    "def extract_additional_info(pdf_path, all_strands_sub_strands):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index, strand_info in enumerate(all_strands_sub_strands):\n",
    "            start_page = strand_info[\"page_number\"]\n",
    "            end_page = all_strands_sub_strands[index + 1][\"page_number\"] if index + 1 < len(all_strands_sub_strands) else len(pdf.pages) + 1\n",
    "            outcomes, experiences, questions = [], [], []\n",
    "            values, community_service, pcis, links_to_subjects, additional_community_service = [], [], [], [], []\n",
    "            skip_remaining_tables = False  # Flag to skip remaining tables once \"Indicators\" is found\n",
    "\n",
    "            for page_number in range(start_page, end_page):\n",
    "                if skip_remaining_tables:\n",
    "                    break  # Move to the next strand_info if \"Indicators\" was found in previous tables\n",
    "\n",
    "                page = pdf.pages[page_number - 1]\n",
    "                tables = page.extract_tables()\n",
    "\n",
    "                for table in tables:\n",
    "                    for row in table:\n",
    "                        row_text = ' '.join(filter(None, row))  # Combine all non-empty cells in the row\n",
    "\n",
    "                        if \"Indicators\" in row_text:\n",
    "                            skip_remaining_tables = True\n",
    "                            break  # Skip to the next strand_info\n",
    "                        for cell in row:\n",
    "                            if cell:\n",
    "                                if \"Values:\" in cell:\n",
    "                                    values.append(cell.split(\"Values:\")[1].strip())\n",
    "                                elif \"Suggested Community Service Learning Activities:\" in cell:\n",
    "                                    community_service.append(cell.split(\"Suggested Community Service Learning Activities:\")[1].strip())\n",
    "                                elif \"PCIs:\" in cell:\n",
    "                                    pcis.append(cell.split(\"PCIs:\")[1].strip())\n",
    "                                elif \"Links to other subjects:\" in cell:\n",
    "                                    links_to_subjects.append(cell.split(\"Links to other subjects:\")[1].strip())\n",
    "                                elif \"Suggested Community Service Learning Activities:\" in cell:\n",
    "                                    additional_community_service.append(cell.split(\"Suggested Community Service Learning Activities:\")[1].strip())\n",
    "\n",
    "                        # Extract other standard information if the row has 5 columns\n",
    "                        if len(row) >= 5:\n",
    "                            outcomes.append(row[2].replace('\\n', ' ').strip() if row[2] else '')\n",
    "                            experiences.append(row[3].replace('\\n', ' ').strip() if row[3] else '')\n",
    "                            questions.append(row[4].replace('\\n', ' ').strip() if row[4] else '')\n",
    "\n",
    "            # Add the collected data to the strand_info\n",
    "            strand_info.update({\n",
    "                \"specific_learning_outcomes\": outcomes,\n",
    "                \"suggested_learning_experiences\": experiences,\n",
    "                \"key_inquiry_questions\": questions,\n",
    "                \"values\": values,\n",
    "                \"community_service_activities\": community_service,\n",
    "                \"pcis\": pcis,\n",
    "                \"links_to_other_subjects\": links_to_subjects,\n",
    "                \"additional_community_service_activities\": additional_community_service\n",
    "            })\n",
    "\n",
    "    return all_strands_sub_strands\n",
    "\n",
    "# Extract and add additional information to the strands and sub_strands data\n",
    "complete_data = extract_additional_info(pdf_path, updated_strands_sub_strands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rubric_data_complete(pdf_path, complete_data, rubric_headers):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index, strand_info in enumerate(complete_data):\n",
    "            rubric_start_page = strand_info.get(\"rubric_start_page\")\n",
    "            next_strand_start_page = complete_data[index + 1][\"page_number\"] if index + 1 < len(complete_data) else len(pdf.pages) + 1\n",
    "            \n",
    "            if rubric_start_page:\n",
    "                rubrics = []\n",
    "                rubric_continues = False\n",
    "                \n",
    "                # Loop through pages starting from rubric_start_page up to the page before next_strand_start_page\n",
    "                for page_number in range(rubric_start_page, next_strand_start_page):\n",
    "                    page = pdf.pages[page_number - 1]\n",
    "                    tables = page.extract_tables()\n",
    "\n",
    "                    # Process each table on the current page\n",
    "                    for table in tables:\n",
    "                        # If it's the first page of the rubrics or if rubric_continues is True\n",
    "                        if rubric_continues or any(header in table[0] for header in rubric_headers):\n",
    "                            # Set rubric_continues to False initially\n",
    "                            rubric_continues = False\n",
    "                            # Process each row in the table as a rubric entry\n",
    "                            for row in table[1:] if any(header in table[0] for header in rubric_headers) else table:\n",
    "                                # Construct the rubric entry if the row has 5 columns\n",
    "                                if len(row) >= 5:\n",
    "                                    rubric_entry = {\n",
    "                                        \"indicator_name\": row[0].replace('\\n', ' ').strip(),\n",
    "                                        \"rubrics\": [\n",
    "                                            {\"level\": \"Exceeds Expectations\", \"statement\": row[1].replace('\\n', ' ').strip()},\n",
    "                                            {\"level\": \"Meets Expectations\", \"statement\": row[2].replace('\\n', ' ').strip()},\n",
    "                                            {\"level\": \"Approaches Expectations\", \"statement\": row[3].replace('\\n', ' ').strip()},\n",
    "                                            {\"level\": \"Below Expectations\", \"statement\": row[4].replace('\\n', ' ').strip()}\n",
    "                                        ]\n",
    "                                    }\n",
    "                                    rubrics.append(rubric_entry)\n",
    "                                \n",
    "                            # If this table was a rubric table, set rubric_continues to True for the next page\n",
    "                            if any(header in table[0] for header in rubric_headers):\n",
    "                                rubric_continues = True\n",
    "\n",
    "                # Update the strand_info with the accumulated rubric entries\n",
    "                strand_info[\"assessment_rubrics\"] = rubrics\n",
    "\n",
    "    return complete_data\n",
    "\n",
    "# Assuming complete_data is already defined and includes rubric start pages\n",
    "# pdf_path = '/mnt/data/GRADE 6 CURRICULUM DESIGNS- MATHEMATICS.pdf'\n",
    "rubric_headers = [\"Indicators\", \"Exceeds Expectations\", \"Meets Expectations\", \"Approaches Expectations\", \"Below Expectations\"]\n",
    "\n",
    "# Extract rubric data\n",
    "complete_data_with_rubrics = extract_rubric_data_complete(pdf_path, complete_data, rubric_headers)\n",
    "\n",
    "# Check the last indicator name for the first rubric entry to verify correct extraction\n",
    "if complete_data_with_rubrics and complete_data_with_rubrics[0][\"assessment_rubrics\"]:\n",
    "    last_indicator = complete_data_with_rubrics[0][\"assessment_rubrics\"][-1][\"indicator_name\"]\n",
    "else:\n",
    "    last_indicator = \"No rubric entries found\"\n",
    "\n",
    "last_indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_data_with_rubrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the path for the processed JSON file\n",
    "original_file_name = \"GRADE 6 CURRICULUM DESIGNS- MATHEMATICS\"\n",
    "processed_pdf_path = f'../data/processed/{original_file_name}.json'\n",
    "\n",
    "# Writing the extracted data to a JSON file\n",
    "with open(processed_pdf_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(complete_data_with_rubrics, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = pdfplumber.open(pdf_path) \n",
    "# page = pdf.pages[16 - 1]\n",
    "# tables = page.extract_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
