{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njui/kn_workspace/curriculum_taxonomy_extractor/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import linktransformer as lt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = \"/home/njui/kn_workspace/curriculum_taxonomy_extractor/data/processed/CBC Math/Grade 4 Curriculum designs Maths CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "# Specify multiple folder paths here\n",
    "folder_paths = [\n",
    "    \"/home/njui/kn_workspace/curriculum_taxonomy_extractor/data/processed/CBC Math/Grade 3 Curriculum designs Maths CSV\",\n",
    "    \"/home/njui/kn_workspace/curriculum_taxonomy_extractor/data/processed/CBC Math/Grade 4 Curriculum designs Maths CSV\",\n",
    "    \"/home/njui/kn_workspace/curriculum_taxonomy_extractor/data/processed/CBC Math/Grade 6 Curriculum designs Maths CSV\",\n",
    "    # Add more paths as needed\n",
    "]\n",
    "\n",
    "# Initialize lists to store the data\n",
    "file_names = []\n",
    "file_paths = []\n",
    "folder_names = []\n",
    "file_details = []\n",
    "indicator_files = []\n",
    "read_status= []\n",
    "\n",
    "# Keywords to check in the CSV contents\n",
    "key_words = [\"meets expectations\", \"below expectations\"]\n",
    "\n",
    "# Iterate through each specified folder path\n",
    "for folder_path in folder_paths:\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Construct the full file path\n",
    "            full_file_path = os.path.join(root, file)\n",
    "            file_names.append(file)\n",
    "            file_paths.append(full_file_path)\n",
    "            folder_name = os.path.basename(root)\n",
    "            folder_names.append(folder_name)\n",
    "\n",
    "            # Read the CSV file and convert it to a JSON string\n",
    "            try:\n",
    "                with open(full_file_path, 'rb') as file:\n",
    "                    result = chardet.detect(file.read())\n",
    "                    encoding = result['encoding']\n",
    "                df_temp = pd.read_csv(full_file_path, encoding=encoding)\n",
    "                details = df_temp.to_json()\n",
    "                file_details.append(details)\n",
    "                \n",
    "                # Check if any of the key words are in the CSV\n",
    "                indicator = any(word in df_temp.to_string().lower() for word in key_words)\n",
    "                indicator_files.append(indicator)\n",
    "                read_status.append(True)\n",
    "            except Exception as e:\n",
    "                file_details.append({'error': str(e)})\n",
    "                read_status.append(False)\n",
    "                indicator_files.append('')  # Default to False in case of error\n",
    "\n",
    "# Create a dataframe with the collected data\n",
    "df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'file_path': file_paths,\n",
    "    'folder_name': folder_names,\n",
    "    'details': file_details,\n",
    "    'indicator_status': indicator_files,\n",
    "    'read_status': read_status\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "distinct_read_status = df['read_status'].unique()\n",
    "print(distinct_read_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No base organization specified, if there is an error, it is likely because of that.\n",
      "Trying to append the model : sentence-transformers/all-MiniLM-L6-v2 and linktransformers/all-MiniLM-L6-v2. Check your path otherwise!\n",
      "Trying sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-05 10:15:27 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 10:15:28 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# # Set your OpenAI API key\n",
    "# openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Specify the model you want to use for generating embeddings or similarity scores\n",
    "# model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# # Specify the clustering type and parameters\n",
    "# cluster_type = \"SLINK\"\n",
    "# cluster_params = {'threshold': 0.90, \"min cluster size\": 2, \"metric\": \"cosine\"}\n",
    "# # cluster_params = {\n",
    "# #     \"max cluster size\": 2,\n",
    "# #     \"threshold\": 0.95,\n",
    "# #     \"max samples\": 2  # Default is set to the same as \"min cluster size\" unless specified.\n",
    "# # }\n",
    "\n",
    "# # Call the cluster_rows function, specifying the column to cluster on\n",
    "# clustered_df = lt.cluster_rows(\n",
    "#     df=df,\n",
    "#     model=model,\n",
    "#     on=['file_name','folder_name'],  # The column to deduplicate on\n",
    "#     cluster_type=cluster_type,\n",
    "#     cluster_params=cluster_params,\n",
    "#     openai_key=openai_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby('indicator_status')\n",
    "\n",
    "# Get the DataFrames for each group\n",
    "df_rubrics = df_grouped.get_group(True)\n",
    "df_strands = df_grouped.get_group(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_strands.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No base organization specified, if there is an error, it is likely because of that.\n",
      "Trying to append the model : sentence-transformers/all-MiniLM-L6-v2 and linktransformers/all-MiniLM-L6-v2. Check your path otherwise!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying sentence-transformers/all-MiniLM-L6-v2...\n",
      "No base organization specified, if there is an error, it is likely because of that.\n",
      "Trying to append the model : sentence-transformers/all-MiniLM-L6-v2 and linktransformers/all-MiniLM-L6-v2. Check your path otherwise!\n",
      "Trying sentence-transformers/all-MiniLM-L6-v2...\n",
      "2024-03-05 13:48:44 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:45 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:45 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:46 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:46 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:46 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:47 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:47 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:47 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:48 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:48 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:49 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:49 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:49 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:50 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:50 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:51 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:51 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:51 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:52 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:52 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:53 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:53 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:53 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:54 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:54 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:55 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-05 13:48:55 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Using the lt.merge function to perform a semantic merge\n",
    "df_merged = lt.merge(\n",
    "    df1=df_strands, \n",
    "    df2=df_rubrics, \n",
    "    on=['file_name','folder_name','details'], \n",
    "    merge_type='1:1',\n",
    "    model='all-MiniLM-L6-v2',\n",
    "    left_on=None, \n",
    "    right_on=None, \n",
    "    suffixes=('_x', '_y'), \n",
    "    use_gpu=False, \n",
    "    batch_size=128, \n",
    "    openai_key=os.getenv('OPENAI_API_KEY')  # Assuming you've set your API key in the environment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sort_values(by='score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
